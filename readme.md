# ğŸš€ ETL Pipeline: Business Central a S3

Este proyecto implementa un pipeline de **ETL (Extract, Transform, Load)** robusto y escalable, diseÃ±ado para extraer datos de la API de **Microsoft Dynamics 365 Business Central**, transformarlos al formato Parquet y cargarlos en un bucket de **AWS S3**.

Todo el proceso estÃ¡ orquestado con **Docker Compose** y utiliza un brÃ³ker de mensajes **RabbitMQ** para desacoplar los servicios y garantizar la fiabilidad en la entrega de datos.

---

## âœ¨ CaracterÃ­sticas Principales

- **ExtracciÃ³n de Datos DinÃ¡mica**: Ingiere datos de mÃºltiples *endpoints* de la API de Business Central, especificados mediante argumentos de lÃ­nea de comandos.
- **AutenticaciÃ³n Segura**: Utiliza OAuth 2.0 (Client Credentials) para autenticarse de forma segura con la API.
- **Procesamiento AsÃ­ncrono**: Emplea **RabbitMQ** como cola de mensajes para gestionar la ingesta de datos de forma asÃ­ncrona, mejorando la escalabilidad y la resiliencia.
- **TransformaciÃ³n Eficiente**: Convierte los datos de formato JSON a **Apache Parquet**, optimizando el almacenamiento y la velocidad de las consultas analÃ­ticas.
- **Carga en la Nube**: Almacena los archivos Parquet procesados en un bucket de **AWS S3**.
- **ContenerizaciÃ³n**: Todo el entorno (aplicaciÃ³n y servicios) estÃ¡ completamente contenerizado con **Docker**, garantizando la portabilidad y la facilidad de despliegue.
- **Logging Centralizado**: ConfiguraciÃ³n de logging centralizada que guarda los registros en ficheros y silencia el ruido de las librerÃ­as de terceros para una depuraciÃ³n mÃ¡s limpia.
- **ConfiguraciÃ³n Flexible**: Gestiona toda la configuraciÃ³n sensible y de entorno a travÃ©s de un fichero `.env`.

---

## ğŸ—ï¸ Arquitectura del Pipeline

El flujo de trabajo se divide en tres componentes principales orquestados por Docker Compose:

1. **Productor (`ingestion_task`)**: Un script de Python (`main_ingestion.py`) que se ejecuta como una tarea. Se encarga de:
   - Obtener un token de autenticaciÃ³n.
   - Realizar peticiones a la API de Business Central para uno o varios *endpoints*.
   - Publicar los datos obtenidos (en formato JSON) en un *exchange* de RabbitMQ.

2. **BrÃ³ker de Mensajes (`rabbitmq`)**: Una instancia de RabbitMQ que actÃºa como intermediario. Recibe los mensajes del productor y los enruta a las colas correspondientes, desacoplando la ingesta de la carga.

3. **Consumidor (`consumer`)**: Un servicio de Python (`consumer.py`) que corre de forma continua. Se encarga de:
   - Escuchar los mensajes de las colas de RabbitMQ.
   - Procesar cada mensaje, convirtiendo los datos JSON a formato Parquet.
   - Subir el fichero `.parquet` resultante al bucket de S3.

```mermaid
graph TD
    A[ğŸ‘¨â€ğŸ’» Usuario ejecuta Ingestion Task] --> B(main_ingestion.py)
    B --> C{API Business Central}
      --> D[Publica en RabbitMQ]
    subgraph "Docker Compose"
        D -- Mensaje JSON --> E(RabbitMQ Exchange)
        E -- Routing Key --> F[Cola de Endpoint]
        G(consumer.py) -- Escucha --> F
    end
    G --> H(Transforma a Parquet)
    H --> I[â˜ï¸ Sube a AWS S3]
```

---

## ğŸ“ Estructura del Proyecto

```
/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ PBI_endpoints.json      # Lista de endpoints de la API a consultar.
â”‚   â””â”€â”€ log_config.py           # MÃ³dulo de configuraciÃ³n centralizada de logging.
â”‚
â”œâ”€â”€ consumer/
â”‚   â””â”€â”€ consumer.py             # LÃ³gica del consumidor: recibe de RabbitMQ, transforma y carga a S3.
â”‚
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ consumer.log            # Logs generados por el consumidor.
â”‚   â””â”€â”€ main_ingestion.log      # Logs generados por la tarea de ingesta.
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ auth/
â”‚   â”‚   â””â”€â”€ token_manager.py    # Gestiona la obtenciÃ³n de tokens OAuth 2.0.
â”‚   â”œâ”€â”€ data_ingestor/
â”‚   â”‚   â””â”€â”€ api_cliente.py      # Cliente para realizar peticiones a la API.
â”‚   â””â”€â”€ messaging/
â”‚       â””â”€â”€ publisher.py        # Publica mensajes en RabbitMQ.
â”‚
â”œâ”€â”€ token/
â”‚   â””â”€â”€ token.json              # Almacena el token de acceso mÃ¡s reciente (ignorado por Git).
â”‚
â”œâ”€â”€ .env                        # Fichero de variables de entorno (debe ser creado localmente).
â”œâ”€â”€ docker-compose.yml          # Define y orquesta los servicios de la aplicaciÃ³n.
â”œâ”€â”€ main_ingestion.py           # Punto de entrada para la tarea de ingesta (productor).
â””â”€â”€ requirements.txt            # Dependencias de Python del proyecto.
```

---

## ğŸš€ CÃ³mo Empezar

Sigue estos pasos para poner en marcha el pipeline en tu entorno local.

### ğŸ”§ Requisitos Previos

- [Docker](https://www.docker.com/get-started)
- [Docker Compose](https://docs.docker.com/compose/install/)

### ğŸ› ï¸ Pasos de InstalaciÃ³n

1. **Clona el repositorio:**

    ```bash
    git clone <URL_DEL_REPOSITORIO>
    cd <NOMBRE_DEL_PROYECTO>
    ```

2. **Crea el fichero de entorno `.env`:**

    ```ini
    # .env

    # Credenciales de la App de Microsoft Entra ID (Azure)
    BC_CLIENT_ID="TU_CLIENT_ID"
    BC_CLIENT_SECRET="TU_CLIENT_SECRET"
    BC_TENANT_ID="TU_TENANT_ID"
    BC_BASE_URL="URL_BASE_DE_LA_API_DE_BUSINESS_CENTRAL"

    # ConfiguraciÃ³n de RabbitMQ
    AMQP_URL="amqp://guest:guest@rabbitmq:5672/"
    EXCHANGE_NAME="API_exchange"

    # ConfiguraciÃ³n de AWS S3
    S3_BUCKET_NAME="NOMBRE_DE_TU_BUCKET_S3"
    AWS_ACCESS_KEY_ID="TU_AWS_ACCESS_KEY"
    AWS_SECRET_ACCESS_KEY="TU_AWS_SECRET_KEY"
    AWS_DEFAULT_REGION="tu-region-de-aws" # ej: eu-west-1
    ```

3. **Construye y levanta los servicios:**

    ```bash
    docker-compose up -d --build
    ```

    El consumidor (`consumer`) se quedarÃ¡ esperando mensajes.

---

## â–¶ï¸ CÃ³mo Usar

Para iniciar la ingesta de datos, ejecuta el servicio `ingestion_task` definido en `docker-compose.yml`.

- **Para ingerir datos de TODOS los *endpoints* definidos en `PBI_endpoints.json`:**

    ```bash
    docker-compose run --rm ingestion_task all
    ```

    *O simplemente:*

    ```bash
    docker-compose run --rm ingestion_task
    ```

- **Para ingerir datos de un ÃšNICO *endpoint*:**

    ```bash
    docker-compose run --rm ingestion_task PBI_Bancos
    ```

    *(Reemplaza `PBI_Bancos` por el endpoint que desees procesar).*

Puedes monitorizar los logs de los servicios con:

```bash
# Ver logs del consumidor
docker-compose logs -f consumer

# Ver logs de la tarea de ingesta (mientras se ejecuta)
docker-compose logs -f ingestion_task
```

---

## ğŸ“¦ Dependencias

Las principales librerÃ­as de Python utilizadas en este proyecto son:

- `pika`: Cliente de Python para RabbitMQ.
- `boto3`: SDK de AWS para Python, utilizado para interactuar con S3.
- `pandas`: Para la manipulaciÃ³n y estructuraciÃ³n de datos.
- `pyarrow`: Motor para la escritura de ficheros en formato Parquet.
- `requests`: Para realizar peticiones HTTP a la API.
- `python-dotenv`: Para cargar variables de entorno desde el fichero `.env`.